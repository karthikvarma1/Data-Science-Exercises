{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes' theorem with the \"naive\" assumption of conditional independence between every pair of features given the value of the class variable. Bayes'theorem states the following relationship, given class variable $y$ and dependent feature vector $x_1$ through $x_n$,:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the naive conditional independence assumption, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i \\mid y)$; the former is then the relative frequency of class $y$ in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*References*:\n",
    "H. Zhang (2004). The optimality of Naive Bayes. Proc. FLAIRS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) implements the Gaussian Naive Bayes algorithm for classification on the data sets where features are continuous.   \n",
    "The likelihood of the features is assumed to be Gaussian:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters $\\sigma_y$ and $\\mu_y$  are estimated using maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo\n",
    "In this demo, we show how to build a Gaussian Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a synthetica 2D dataset\n",
    "X, y = make_classification(n_samples=50, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_classes=3, n_clusters_per_class=1, \n",
    "                           weights=None, flip_y=0.01, class_sep=0.5, hypercube=True,\n",
    "                           shift=0.0, scale=1.0, shuffle=True, random_state=42)\n",
    "\n",
    "# Data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "# Visualize the generated data\n",
    "colors = ['blue', 'yellow', 'green']\n",
    "for i, color in enumerate(colors):\n",
    "    plt.scatter(X_train[y_train == i, 0], X_train[y_train == i, 1], c=color)\n",
    "plt.scatter(X_test[:, 0], X_test[:,1], c='red', marker='x', label='Testing Data')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is:  0.8\n"
     ]
    }
   ],
   "source": [
    "# Create and training a Gaussian Naive Bayes classifier model\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to predict testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Testing accuracy is: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.325 0.375 0.3  ]\n",
      "Estimated mean for each Gaussian distribution: \n",
      " [[ 0.60903899 -0.56115715]\n",
      " [ 0.39670288  0.51301944]\n",
      " [-0.40161257 -0.83685934]]\n",
      "Estimated variance for each Gaussian distribution: \n",
      " [[0.23233913 1.0483911 ]\n",
      " [0.93521181 0.0662763 ]\n",
      " [0.3309853  0.6755908 ]]\n"
     ]
    }
   ],
   "source": [
    "# Explore the learned probability (model parameters)\n",
    "print('Estimated probability of classess: \\n', clf.class_prior_)\n",
    "print('Estimated mean for each Gaussian distribution: \\n', clf.theta_)\n",
    "print('Estimated variance for each Gaussian distribution: \\n', clf.sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for Class 0 and the first feature, we can have the following Gaussian disribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_0 \\mid Class=0) = \\frac{1}{\\sqrt{2\\pi\\cdot0.2323}} \\exp\\left(-\\frac{(x_0 - 0.6090)^2}{2\\cdot0.2323}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes accuracy: 0.7000 +- 0.1612\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use 10-fold cross validation to show a more robust prediction accuracy\n",
    "clf = GaussianNB()\n",
    "scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
    "print('Gaussian Naive Bayes accuracy: %.4f +- %.4f\\n' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks** - The training data is generated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means estaimated manually: \n",
      " [[-2.         -1.33333333]\n",
      " [ 2.          1.33333333]]\n",
      "Variances estaimated manually: \n",
      " [[0.66666667 0.22222222]\n",
      " [0.66666667 0.22222222]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "# Firstly, let's do the parameter estimation manually without using the model\n",
    "X_0_C_1=X[y==1][:,0]\n",
    "X_1_C_1=X[y==1][:,1]\n",
    "X_0_C_2=X[y==2][:,0]\n",
    "X_1_C_2=X[y==2][:,1]\n",
    "\n",
    "manual_means = np.array([[X_0_C_1.mean(), X_1_C_1.mean()], [X_0_C_2.mean(), X_1_C_2.mean()]])\n",
    "print('Means estaimated manually: \\n', manual_means)\n",
    "manual_vars = np.array([[X_0_C_1.var(), X_1_C_1.var()], [X_0_C_2.var(), X_1_C_2.var()]])\n",
    "print('Variances estaimated manually: \\n', manual_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**: Training a GaussianNB model and print out the learned model parameters (parameters of probability distributions). And check if the learned parameters comply with the manually estimated ones as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Create and training a Gaussian Naive Bayes classifier model\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, y)\n",
    "# Using the model to predict the data\n",
    "y_pred = clf.predict(X)\n",
    "accuracy = accuracy_score(y_pred, y)\n",
    "print('Accuracy is: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated mean for each Gaussian distribution: \n",
      " [[-2.         -1.33333333]\n",
      " [ 2.          1.33333333]]\n",
      "Estimated variance for each Gaussian distribution: \n",
      " [[0.66666667 0.22222223]\n",
      " [0.66666667 0.22222223]]\n"
     ]
    }
   ],
   "source": [
    "# Exploring the learned probability (model parameters)\n",
    "print('Estimated mean for each Gaussian distribution: \\n', clf.theta_)\n",
    "print('Estimated variance for each Gaussian distribution: \\n', clf.sigma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**: Predict the label of a data [-0.8,-1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label for the data is: [1]\n"
     ]
    }
   ],
   "source": [
    "print('The label for the data is:', clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) classification model is suitable for classification with discrete features. To let the model handle to categorical data, we often need to transform the categorical values to numberic ones, through [encoding](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Outlook Humidity    Wind Play\n",
      "0     Sunny     High    Weak   No\n",
      "1     Sunny     High  Strong   No\n",
      "2  Overcast     High    Weak  Yes\n",
      "3      Rain     High    Weak  Yes\n",
      "4      Rain   Normal    Weak  Yes\n",
      "\n",
      "Data shape:  (14, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load the weather data\n",
    "weather_data = pd.read_csv('files/weather.csv')\n",
    "print(weather_data.head())\n",
    "print('\\nData shape: ', weather_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre-processing and preparation\n",
    "# Firstly, we need to convert the date from being categorical to being numerical\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "coded_data = enc.fit_transform(weather_data)\n",
    "\n",
    "X = coded_data[:, 0:-1]\n",
    "y = coded_data[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat and train a model\n",
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred = clf_mnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy is: %.2f\\n' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.4 0.6]\n",
      "Estimated class-conditional probabilities for each feature: \n",
      " [[0.63636364 0.18181818 0.18181818]\n",
      " [0.41176471 0.29411765 0.29411765]]\n"
     ]
    }
   ],
   "source": [
    "# Explore the learned model parameters (probabilities)\n",
    "# Note that the probabilities are in the logorithmic form. Why? The log-sum-exp trick for underflow of probability products\n",
    "print('Estimated probability of classess: \\n', np.e**clf_mnb.class_log_prior_)\n",
    "print('Estimated class-conditional probabilities for each feature: \\n', np.e**clf_mnb.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks** - The training data is generated as follows. The number of data instances (6) is small while the demensionality of the data is relatively highly (100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic data set\n",
    "import numpy as np\n",
    "X = np.random.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**: Training a MultinomialNB model, and predict the label of a data X_new = [[1,2,1,0,2,3,0,3,2,1,1,3,3,0,4,2,2,0,0,2,2,3,4,4,4,4,0,3,3,\n",
    "          1,1,1,2,3,1,3,0,2,2,0,4,2,4,3,2,0,1,1,1,2,3,0,0,3,4,3,3,4,\n",
    "          2,1,0,0,0,0,4,1,2,0,0,4,4,0,4,1,3,1,1,1,3,1,1,1,4,3,1,1,3,\n",
    "          2,0,0,0,3,4,1,1,4,3,2,3,4]]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label for the new data is: [6]\n"
     ]
    }
   ],
   "source": [
    "# Training a MultinomialNB model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict the class of the new data instance\n",
    "X_new = [[1,2,1,0,2,3,0,3,2,1,1,3,3,0,4,2,2,0,0,2,2,3,4,4,4,4,0,3,3,\n",
    "          1,1,1,2,3,1,3,0,2,2,0,4,2,4,3,2,0,1,1,1,2,3,0,0,3,4,3,3,4,\n",
    "          2,1,0,0,0,0,4,1,2,0,0,4,4,0,4,1,3,1,1,1,3,1,1,1,4,3,1,1,3,\n",
    "          2,0,0,0,3,4,1,1,4,3,2,3,4]]\n",
    "print('The label for the new data is:', clf.predict(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**: In our lecture, we discussed that if there is no occurence of some feature values, zero probabilities will appear. To overcome this issue, Laplace correction (smoothing) is proposed, as shown in the follow formula. In the [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) implementation, the parameter 'alpha' controls the way we apply Laplace smoothing. The default value is 'alpha=1.0'. Please create and train a model with no Laplace smoothing for the above data set. Check if there are zero probabilities (note that due to the accuracy issue, zero might be represented as a signficantly small number by the computer), and compare the leaned model parameters (probabilities) with the case 'alpha=1' \n",
    "$$p(x_{yi}|y)=\\frac{N_{yi}+\\alpha}{N_y+{\\alpha}n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train a MultinomialNB model with no Laplace smoothing\n",
    "clf = MultinomialNB(alpha = 0)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      "Estimated class-conditional probabilities for each feature: \n",
      " [[1.91387560e-02 4.78468899e-13 9.56937799e-03 9.56937799e-03\n",
      "  1.43540670e-02 4.78468900e-03 1.91387560e-02 4.78468900e-03\n",
      "  4.78468900e-03 4.78468900e-03 1.91387560e-02 9.56937799e-03\n",
      "  4.78468900e-03 1.43540670e-02 1.91387560e-02 9.56937799e-03\n",
      "  1.43540670e-02 4.78468900e-03 9.56937799e-03 1.43540670e-02\n",
      "  1.43540670e-02 9.56937799e-03 4.78468899e-13 1.91387560e-02\n",
      "  1.43540670e-02 4.78468900e-03 1.91387560e-02 4.78468899e-13\n",
      "  1.91387560e-02 1.43540670e-02 4.78468899e-13 1.43540670e-02\n",
      "  9.56937799e-03 9.56937799e-03 9.56937799e-03 4.78468900e-03\n",
      "  1.43540670e-02 1.91387560e-02 4.78468900e-03 9.56937799e-03\n",
      "  1.91387560e-02 4.78468900e-03 1.43540670e-02 1.91387560e-02\n",
      "  1.91387560e-02 1.43540670e-02 1.91387560e-02 4.78468900e-03\n",
      "  1.43540670e-02 1.91387560e-02 1.43540670e-02 4.78468900e-03\n",
      "  4.78468900e-03 9.56937799e-03 9.56937799e-03 1.91387560e-02\n",
      "  9.56937799e-03 4.78468899e-13 1.91387560e-02 9.56937799e-03\n",
      "  1.91387560e-02 1.91387560e-02 1.91387560e-02 4.78468899e-13\n",
      "  1.91387560e-02 4.78468899e-13 4.78468899e-13 9.56937799e-03\n",
      "  4.78468900e-03 9.56937799e-03 9.56937799e-03 4.78468899e-13\n",
      "  4.78468900e-03 4.78468899e-13 4.78468899e-13 1.91387560e-02\n",
      "  9.56937799e-03 4.78468900e-03 1.43540670e-02 9.56937799e-03\n",
      "  9.56937799e-03 1.91387560e-02 1.43540670e-02 4.78468900e-03\n",
      "  9.56937799e-03 9.56937799e-03 1.43540670e-02 1.91387560e-02\n",
      "  4.78468899e-13 9.56937799e-03 1.43540670e-02 4.78468899e-13\n",
      "  4.78468899e-13 4.78468900e-03 4.78468899e-13 1.43540670e-02\n",
      "  4.78468899e-13 4.78468899e-13 9.56937799e-03 4.78468900e-03]\n",
      " [4.90196078e-03 1.47058824e-02 4.90196078e-03 4.90196078e-03\n",
      "  9.80392157e-03 4.90196078e-03 4.90196078e-13 1.96078431e-02\n",
      "  4.90196078e-03 1.96078431e-02 9.80392157e-03 4.90196078e-03\n",
      "  1.96078431e-02 4.90196078e-03 1.47058824e-02 4.90196078e-03\n",
      "  1.96078431e-02 1.47058824e-02 1.47058824e-02 1.47058824e-02\n",
      "  9.80392157e-03 4.90196078e-03 1.47058824e-02 4.90196078e-13\n",
      "  1.96078431e-02 4.90196078e-13 4.90196078e-13 1.96078431e-02\n",
      "  4.90196078e-03 1.96078431e-02 1.96078431e-02 9.80392157e-03\n",
      "  4.90196078e-13 1.47058824e-02 1.96078431e-02 1.47058824e-02\n",
      "  4.90196078e-03 9.80392157e-03 1.96078431e-02 1.47058824e-02\n",
      "  4.90196078e-13 4.90196078e-03 1.47058824e-02 4.90196078e-03\n",
      "  1.47058824e-02 1.47058824e-02 1.96078431e-02 1.47058824e-02\n",
      "  9.80392157e-03 1.47058824e-02 1.96078431e-02 4.90196078e-03\n",
      "  4.90196078e-03 1.47058824e-02 1.47058824e-02 4.90196078e-13\n",
      "  4.90196078e-03 1.47058824e-02 4.90196078e-13 1.47058824e-02\n",
      "  1.96078431e-02 4.90196078e-03 4.90196078e-13 4.90196078e-03\n",
      "  4.90196078e-03 1.47058824e-02 4.90196078e-03 4.90196078e-03\n",
      "  4.90196078e-13 1.47058824e-02 1.47058824e-02 1.96078431e-02\n",
      "  4.90196078e-13 9.80392157e-03 4.90196078e-03 1.47058824e-02\n",
      "  1.47058824e-02 1.47058824e-02 4.90196078e-03 1.96078431e-02\n",
      "  4.90196078e-03 9.80392157e-03 4.90196078e-03 1.47058824e-02\n",
      "  4.90196078e-13 4.90196078e-03 1.47058824e-02 9.80392157e-03\n",
      "  9.80392157e-03 4.90196078e-13 4.90196078e-13 1.47058824e-02\n",
      "  4.90196078e-13 1.96078431e-02 1.96078431e-02 4.90196078e-03\n",
      "  4.90196078e-13 1.47058824e-02 9.80392157e-03 1.47058824e-02]\n",
      " [1.49253731e-02 4.97512438e-03 1.99004975e-02 1.49253731e-02\n",
      "  4.97512438e-03 4.97512438e-03 4.97512438e-13 4.97512438e-13\n",
      "  4.97512438e-03 9.95024876e-03 4.97512438e-03 9.95024876e-03\n",
      "  1.49253731e-02 4.97512438e-13 4.97512438e-13 1.49253731e-02\n",
      "  1.49253731e-02 1.49253731e-02 4.97512438e-03 9.95024876e-03\n",
      "  1.49253731e-02 4.97512438e-03 4.97512438e-13 4.97512438e-13\n",
      "  1.99004975e-02 4.97512438e-13 4.97512438e-13 4.97512438e-13\n",
      "  1.99004975e-02 1.49253731e-02 4.97512438e-13 1.49253731e-02\n",
      "  4.97512438e-03 4.97512438e-13 1.99004975e-02 4.97512438e-03\n",
      "  4.97512438e-03 1.99004975e-02 4.97512438e-03 1.49253731e-02\n",
      "  1.99004975e-02 1.49253731e-02 1.99004975e-02 1.49253731e-02\n",
      "  4.97512438e-03 4.97512438e-13 9.95024876e-03 1.49253731e-02\n",
      "  4.97512438e-03 4.97512438e-03 4.97512438e-03 4.97512438e-03\n",
      "  1.49253731e-02 1.49253731e-02 9.95024876e-03 9.95024876e-03\n",
      "  9.95024876e-03 4.97512438e-03 4.97512438e-03 9.95024876e-03\n",
      "  1.99004975e-02 1.49253731e-02 1.49253731e-02 4.97512438e-03\n",
      "  1.49253731e-02 1.49253731e-02 1.99004975e-02 1.99004975e-02\n",
      "  1.99004975e-02 1.49253731e-02 1.49253731e-02 4.97512438e-03\n",
      "  4.97512438e-13 1.99004975e-02 1.99004975e-02 4.97512438e-03\n",
      "  1.49253731e-02 9.95024876e-03 4.97512438e-03 4.97512438e-03\n",
      "  4.97512438e-03 9.95024876e-03 1.99004975e-02 9.95024876e-03\n",
      "  1.99004975e-02 1.99004975e-02 9.95024876e-03 1.49253731e-02\n",
      "  1.99004975e-02 4.97512438e-13 9.95024876e-03 4.97512438e-03\n",
      "  4.97512438e-13 4.97512438e-03 1.49253731e-02 4.97512438e-13\n",
      "  1.99004975e-02 1.49253731e-02 4.97512438e-03 4.97512438e-03]\n",
      " [1.35135135e-02 4.50450450e-03 4.50450450e-03 1.80180180e-02\n",
      "  4.50450450e-13 1.35135135e-02 1.35135135e-02 1.80180180e-02\n",
      "  4.50450450e-13 4.50450450e-13 1.35135135e-02 1.80180180e-02\n",
      "  4.50450450e-13 1.35135135e-02 1.80180180e-02 1.35135135e-02\n",
      "  9.00900901e-03 1.35135135e-02 4.50450450e-13 1.80180180e-02\n",
      "  1.80180180e-02 9.00900901e-03 9.00900901e-03 1.35135135e-02\n",
      "  4.50450450e-13 4.50450450e-03 9.00900901e-03 4.50450450e-13\n",
      "  4.50450450e-03 4.50450450e-13 4.50450450e-03 1.80180180e-02\n",
      "  9.00900901e-03 1.80180180e-02 4.50450450e-03 9.00900901e-03\n",
      "  1.80180180e-02 4.50450450e-03 4.50450450e-13 4.50450450e-13\n",
      "  1.35135135e-02 4.50450450e-03 4.50450450e-03 1.80180180e-02\n",
      "  9.00900901e-03 1.80180180e-02 4.50450450e-13 1.35135135e-02\n",
      "  1.35135135e-02 1.80180180e-02 9.00900901e-03 4.50450450e-13\n",
      "  1.80180180e-02 4.50450450e-03 1.80180180e-02 4.50450450e-13\n",
      "  1.80180180e-02 1.35135135e-02 4.50450450e-13 1.80180180e-02\n",
      "  1.80180180e-02 4.50450450e-03 1.35135135e-02 4.50450450e-13\n",
      "  1.80180180e-02 1.80180180e-02 4.50450450e-03 1.80180180e-02\n",
      "  1.35135135e-02 4.50450450e-13 4.50450450e-03 4.50450450e-03\n",
      "  1.80180180e-02 1.35135135e-02 1.35135135e-02 1.35135135e-02\n",
      "  1.80180180e-02 9.00900901e-03 4.50450450e-03 1.35135135e-02\n",
      "  9.00900901e-03 9.00900901e-03 4.50450450e-13 4.50450450e-03\n",
      "  4.50450450e-03 4.50450450e-03 1.35135135e-02 1.35135135e-02\n",
      "  1.80180180e-02 9.00900901e-03 9.00900901e-03 1.35135135e-02\n",
      "  1.35135135e-02 4.50450450e-03 1.80180180e-02 1.80180180e-02\n",
      "  1.35135135e-02 1.35135135e-02 1.80180180e-02 4.50450450e-03]\n",
      " [1.96078431e-02 9.80392157e-03 9.80392157e-03 4.90196078e-13\n",
      "  1.47058824e-02 1.96078431e-02 1.47058824e-02 4.90196078e-13\n",
      "  4.90196078e-03 1.96078431e-02 9.80392157e-03 9.80392157e-03\n",
      "  1.47058824e-02 9.80392157e-03 1.96078431e-02 1.96078431e-02\n",
      "  9.80392157e-03 4.90196078e-03 4.90196078e-03 9.80392157e-03\n",
      "  9.80392157e-03 9.80392157e-03 4.90196078e-03 1.96078431e-02\n",
      "  1.96078431e-02 1.96078431e-02 4.90196078e-13 4.90196078e-13\n",
      "  4.90196078e-13 4.90196078e-13 1.47058824e-02 1.96078431e-02\n",
      "  1.47058824e-02 1.96078431e-02 4.90196078e-13 9.80392157e-03\n",
      "  1.47058824e-02 4.90196078e-13 4.90196078e-13 1.96078431e-02\n",
      "  1.96078431e-02 4.90196078e-03 1.96078431e-02 4.90196078e-03\n",
      "  1.96078431e-02 1.47058824e-02 9.80392157e-03 9.80392157e-03\n",
      "  4.90196078e-03 9.80392157e-03 4.90196078e-13 4.90196078e-13\n",
      "  9.80392157e-03 1.47058824e-02 9.80392157e-03 4.90196078e-13\n",
      "  9.80392157e-03 4.90196078e-03 9.80392157e-03 1.47058824e-02\n",
      "  1.96078431e-02 1.47058824e-02 1.47058824e-02 1.47058824e-02\n",
      "  1.47058824e-02 4.90196078e-13 1.96078431e-02 9.80392157e-03\n",
      "  1.47058824e-02 4.90196078e-03 9.80392157e-03 4.90196078e-03\n",
      "  4.90196078e-03 4.90196078e-13 4.90196078e-03 1.47058824e-02\n",
      "  9.80392157e-03 4.90196078e-03 9.80392157e-03 4.90196078e-13\n",
      "  1.96078431e-02 9.80392157e-03 4.90196078e-13 4.90196078e-03\n",
      "  1.96078431e-02 1.96078431e-02 4.90196078e-13 4.90196078e-13\n",
      "  4.90196078e-13 4.90196078e-03 1.96078431e-02 4.90196078e-03\n",
      "  1.96078431e-02 1.96078431e-02 9.80392157e-03 9.80392157e-03\n",
      "  4.90196078e-03 9.80392157e-03 1.47058824e-02 4.90196078e-03]\n",
      " [4.85436893e-03 4.85436893e-03 4.85436893e-13 1.45631068e-02\n",
      "  4.85436893e-13 9.70873786e-03 4.85436893e-13 4.85436893e-13\n",
      "  1.94174757e-02 4.85436893e-03 9.70873786e-03 4.85436893e-03\n",
      "  4.85436893e-03 4.85436893e-13 1.94174757e-02 1.94174757e-02\n",
      "  4.85436893e-03 4.85436893e-13 4.85436893e-13 4.85436893e-03\n",
      "  4.85436893e-03 1.45631068e-02 4.85436893e-03 1.45631068e-02\n",
      "  1.94174757e-02 9.70873786e-03 9.70873786e-03 9.70873786e-03\n",
      "  4.85436893e-13 1.94174757e-02 9.70873786e-03 4.85436893e-13\n",
      "  9.70873786e-03 1.45631068e-02 9.70873786e-03 1.94174757e-02\n",
      "  1.45631068e-02 1.94174757e-02 1.45631068e-02 1.45631068e-02\n",
      "  4.85436893e-03 1.45631068e-02 1.94174757e-02 1.94174757e-02\n",
      "  4.85436893e-13 4.85436893e-13 1.94174757e-02 4.85436893e-03\n",
      "  1.94174757e-02 1.94174757e-02 1.94174757e-02 4.85436893e-13\n",
      "  4.85436893e-03 4.85436893e-13 9.70873786e-03 1.45631068e-02\n",
      "  9.70873786e-03 1.45631068e-02 4.85436893e-13 4.85436893e-03\n",
      "  9.70873786e-03 9.70873786e-03 1.94174757e-02 1.45631068e-02\n",
      "  4.85436893e-03 4.85436893e-13 1.94174757e-02 1.45631068e-02\n",
      "  1.94174757e-02 1.94174757e-02 1.94174757e-02 1.94174757e-02\n",
      "  1.94174757e-02 1.94174757e-02 4.85436893e-03 9.70873786e-03\n",
      "  1.45631068e-02 4.85436893e-13 4.85436893e-03 4.85436893e-03\n",
      "  4.85436893e-03 4.85436893e-13 9.70873786e-03 1.94174757e-02\n",
      "  1.45631068e-02 1.45631068e-02 4.85436893e-03 9.70873786e-03\n",
      "  1.94174757e-02 9.70873786e-03 9.70873786e-03 1.94174757e-02\n",
      "  1.45631068e-02 9.70873786e-03 1.45631068e-02 4.85436893e-03\n",
      "  4.85436893e-03 4.85436893e-03 4.85436893e-13 4.85436893e-03]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X)\n",
    "print('Estimated probability of classess: \\n', np.e**clf.class_log_prior_)\n",
    "print('Estimated class-conditional probabilities for each feature: \\n', np.e**clf.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n",
      "Estimated class-conditional probabilities for each feature: \n",
      " [[0.01618123 0.00323625 0.00970874 0.00970874 0.01294498 0.00647249\n",
      "  0.01618123 0.00647249 0.00647249 0.00647249 0.01618123 0.00970874\n",
      "  0.00647249 0.01294498 0.01618123 0.00970874 0.01294498 0.00647249\n",
      "  0.00970874 0.01294498 0.01294498 0.00970874 0.00323625 0.01618123\n",
      "  0.01294498 0.00647249 0.01618123 0.00323625 0.01618123 0.01294498\n",
      "  0.00323625 0.01294498 0.00970874 0.00970874 0.00970874 0.00647249\n",
      "  0.01294498 0.01618123 0.00647249 0.00970874 0.01618123 0.00647249\n",
      "  0.01294498 0.01618123 0.01618123 0.01294498 0.01618123 0.00647249\n",
      "  0.01294498 0.01618123 0.01294498 0.00647249 0.00647249 0.00970874\n",
      "  0.00970874 0.01618123 0.00970874 0.00323625 0.01618123 0.00970874\n",
      "  0.01618123 0.01618123 0.01618123 0.00323625 0.01618123 0.00323625\n",
      "  0.00323625 0.00970874 0.00647249 0.00970874 0.00970874 0.00323625\n",
      "  0.00647249 0.00323625 0.00323625 0.01618123 0.00970874 0.00647249\n",
      "  0.01294498 0.00970874 0.00970874 0.01618123 0.01294498 0.00647249\n",
      "  0.00970874 0.00970874 0.01294498 0.01618123 0.00323625 0.00970874\n",
      "  0.01294498 0.00323625 0.00323625 0.00647249 0.00323625 0.01294498\n",
      "  0.00323625 0.00323625 0.00970874 0.00647249]\n",
      " [0.00657895 0.01315789 0.00657895 0.00657895 0.00986842 0.00657895\n",
      "  0.00328947 0.01644737 0.00657895 0.01644737 0.00986842 0.00657895\n",
      "  0.01644737 0.00657895 0.01315789 0.00657895 0.01644737 0.01315789\n",
      "  0.01315789 0.01315789 0.00986842 0.00657895 0.01315789 0.00328947\n",
      "  0.01644737 0.00328947 0.00328947 0.01644737 0.00657895 0.01644737\n",
      "  0.01644737 0.00986842 0.00328947 0.01315789 0.01644737 0.01315789\n",
      "  0.00657895 0.00986842 0.01644737 0.01315789 0.00328947 0.00657895\n",
      "  0.01315789 0.00657895 0.01315789 0.01315789 0.01644737 0.01315789\n",
      "  0.00986842 0.01315789 0.01644737 0.00657895 0.00657895 0.01315789\n",
      "  0.01315789 0.00328947 0.00657895 0.01315789 0.00328947 0.01315789\n",
      "  0.01644737 0.00657895 0.00328947 0.00657895 0.00657895 0.01315789\n",
      "  0.00657895 0.00657895 0.00328947 0.01315789 0.01315789 0.01644737\n",
      "  0.00328947 0.00986842 0.00657895 0.01315789 0.01315789 0.01315789\n",
      "  0.00657895 0.01644737 0.00657895 0.00986842 0.00657895 0.01315789\n",
      "  0.00328947 0.00657895 0.01315789 0.00986842 0.00986842 0.00328947\n",
      "  0.00328947 0.01315789 0.00328947 0.01644737 0.01644737 0.00657895\n",
      "  0.00328947 0.01315789 0.00986842 0.01315789]\n",
      " [0.01328904 0.00664452 0.0166113  0.01328904 0.00664452 0.00664452\n",
      "  0.00332226 0.00332226 0.00664452 0.00996678 0.00664452 0.00996678\n",
      "  0.01328904 0.00332226 0.00332226 0.01328904 0.01328904 0.01328904\n",
      "  0.00664452 0.00996678 0.01328904 0.00664452 0.00332226 0.00332226\n",
      "  0.0166113  0.00332226 0.00332226 0.00332226 0.0166113  0.01328904\n",
      "  0.00332226 0.01328904 0.00664452 0.00332226 0.0166113  0.00664452\n",
      "  0.00664452 0.0166113  0.00664452 0.01328904 0.0166113  0.01328904\n",
      "  0.0166113  0.01328904 0.00664452 0.00332226 0.00996678 0.01328904\n",
      "  0.00664452 0.00664452 0.00664452 0.00664452 0.01328904 0.01328904\n",
      "  0.00996678 0.00996678 0.00996678 0.00664452 0.00664452 0.00996678\n",
      "  0.0166113  0.01328904 0.01328904 0.00664452 0.01328904 0.01328904\n",
      "  0.0166113  0.0166113  0.0166113  0.01328904 0.01328904 0.00664452\n",
      "  0.00332226 0.0166113  0.0166113  0.00664452 0.01328904 0.00996678\n",
      "  0.00664452 0.00664452 0.00664452 0.00996678 0.0166113  0.00996678\n",
      "  0.0166113  0.0166113  0.00996678 0.01328904 0.0166113  0.00332226\n",
      "  0.00996678 0.00664452 0.00332226 0.00664452 0.01328904 0.00332226\n",
      "  0.0166113  0.01328904 0.00664452 0.00664452]\n",
      " [0.01242236 0.00621118 0.00621118 0.01552795 0.00310559 0.01242236\n",
      "  0.01242236 0.01552795 0.00310559 0.00310559 0.01242236 0.01552795\n",
      "  0.00310559 0.01242236 0.01552795 0.01242236 0.00931677 0.01242236\n",
      "  0.00310559 0.01552795 0.01552795 0.00931677 0.00931677 0.01242236\n",
      "  0.00310559 0.00621118 0.00931677 0.00310559 0.00621118 0.00310559\n",
      "  0.00621118 0.01552795 0.00931677 0.01552795 0.00621118 0.00931677\n",
      "  0.01552795 0.00621118 0.00310559 0.00310559 0.01242236 0.00621118\n",
      "  0.00621118 0.01552795 0.00931677 0.01552795 0.00310559 0.01242236\n",
      "  0.01242236 0.01552795 0.00931677 0.00310559 0.01552795 0.00621118\n",
      "  0.01552795 0.00310559 0.01552795 0.01242236 0.00310559 0.01552795\n",
      "  0.01552795 0.00621118 0.01242236 0.00310559 0.01552795 0.01552795\n",
      "  0.00621118 0.01552795 0.01242236 0.00310559 0.00621118 0.00621118\n",
      "  0.01552795 0.01242236 0.01242236 0.01242236 0.01552795 0.00931677\n",
      "  0.00621118 0.01242236 0.00931677 0.00931677 0.00310559 0.00621118\n",
      "  0.00621118 0.00621118 0.01242236 0.01242236 0.01552795 0.00931677\n",
      "  0.00931677 0.01242236 0.01242236 0.00621118 0.01552795 0.01552795\n",
      "  0.01242236 0.01242236 0.01552795 0.00621118]\n",
      " [0.01644737 0.00986842 0.00986842 0.00328947 0.01315789 0.01644737\n",
      "  0.01315789 0.00328947 0.00657895 0.01644737 0.00986842 0.00986842\n",
      "  0.01315789 0.00986842 0.01644737 0.01644737 0.00986842 0.00657895\n",
      "  0.00657895 0.00986842 0.00986842 0.00986842 0.00657895 0.01644737\n",
      "  0.01644737 0.01644737 0.00328947 0.00328947 0.00328947 0.00328947\n",
      "  0.01315789 0.01644737 0.01315789 0.01644737 0.00328947 0.00986842\n",
      "  0.01315789 0.00328947 0.00328947 0.01644737 0.01644737 0.00657895\n",
      "  0.01644737 0.00657895 0.01644737 0.01315789 0.00986842 0.00986842\n",
      "  0.00657895 0.00986842 0.00328947 0.00328947 0.00986842 0.01315789\n",
      "  0.00986842 0.00328947 0.00986842 0.00657895 0.00986842 0.01315789\n",
      "  0.01644737 0.01315789 0.01315789 0.01315789 0.01315789 0.00328947\n",
      "  0.01644737 0.00986842 0.01315789 0.00657895 0.00986842 0.00657895\n",
      "  0.00657895 0.00328947 0.00657895 0.01315789 0.00986842 0.00657895\n",
      "  0.00986842 0.00328947 0.01644737 0.00986842 0.00328947 0.00657895\n",
      "  0.01644737 0.01644737 0.00328947 0.00328947 0.00328947 0.00657895\n",
      "  0.01644737 0.00657895 0.01644737 0.01644737 0.00986842 0.00986842\n",
      "  0.00657895 0.00986842 0.01315789 0.00657895]\n",
      " [0.00653595 0.00653595 0.00326797 0.0130719  0.00326797 0.00980392\n",
      "  0.00326797 0.00326797 0.01633987 0.00653595 0.00980392 0.00653595\n",
      "  0.00653595 0.00326797 0.01633987 0.01633987 0.00653595 0.00326797\n",
      "  0.00326797 0.00653595 0.00653595 0.0130719  0.00653595 0.0130719\n",
      "  0.01633987 0.00980392 0.00980392 0.00980392 0.00326797 0.01633987\n",
      "  0.00980392 0.00326797 0.00980392 0.0130719  0.00980392 0.01633987\n",
      "  0.0130719  0.01633987 0.0130719  0.0130719  0.00653595 0.0130719\n",
      "  0.01633987 0.01633987 0.00326797 0.00326797 0.01633987 0.00653595\n",
      "  0.01633987 0.01633987 0.01633987 0.00326797 0.00653595 0.00326797\n",
      "  0.00980392 0.0130719  0.00980392 0.0130719  0.00326797 0.00653595\n",
      "  0.00980392 0.00980392 0.01633987 0.0130719  0.00653595 0.00326797\n",
      "  0.01633987 0.0130719  0.01633987 0.01633987 0.01633987 0.01633987\n",
      "  0.01633987 0.01633987 0.00653595 0.00980392 0.0130719  0.00326797\n",
      "  0.00653595 0.00653595 0.00653595 0.00326797 0.00980392 0.01633987\n",
      "  0.0130719  0.0130719  0.00653595 0.00980392 0.01633987 0.00980392\n",
      "  0.00980392 0.01633987 0.0130719  0.00980392 0.0130719  0.00653595\n",
      "  0.00653595 0.00653595 0.00326797 0.00653595]]\n"
     ]
    }
   ],
   "source": [
    "# with laplace smoothing i.e., alpha = 1\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print('Estimated probability of classess: \\n', np.e**clf.class_log_prior_)\n",
    "print('Estimated class-conditional probabilities for each feature: \\n', np.e**clf.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Process on 'Iris' Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Week 9, we have studied how to use KNN algorithm to do classification task on 'iris' data. Here,we are going to employ the GaussianNB to conduct the same task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data['data'], iris_data['target'], random_state=0)\n",
    "X = iris_data.data\n",
    "y = iris_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**ï¼šReport the prediction result on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Train a model and do the prediction\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to predict testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Testing accuracy is: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.33035714 0.30357143 0.36607143]\n",
      "Estimated mean for each Gaussian distribution: \n",
      " [[4.9972973  3.38918919 1.45405405 0.24054054]\n",
      " [5.91764706 2.75882353 4.19117647 1.30882353]\n",
      " [6.66341463 2.9902439  5.58292683 2.03902439]]\n",
      "Estimated variance for each Gaussian distribution: \n",
      " [[0.12242513 0.14474799 0.01978087 0.01159971]\n",
      " [0.2649827  0.11124568 0.22139274 0.0408045 ]\n",
      " [0.4071981  0.11453897 0.30483046 0.06579417]]\n"
     ]
    }
   ],
   "source": [
    "# Explore the learned probability (model parameters)\n",
    "print('Estimated probability of classess: \\n', clf.class_prior_)\n",
    "print('Estimated mean for each Gaussian distribution: \\n', clf.theta_)\n",
    "print('Estimated variance for each Gaussian distribution: \\n', clf.sigma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes accuracy: 0.9533 +- 0.0427\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "scores = cross_val_score(clf, X, y, scoring='accuracy', cv=10)\n",
    "print('Gaussian Naive Bayes accuracy: %.4f +- %.4f\\n' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5**: Compare the prediction accuaracy between KNN clasifier (use the optimal K you've identied) and Gaussian Naive Bayes. Use 10-cross validation to report the accuracy mean and standard deviation (Note this is to ensure the comparison is based on robust performace). Which classifidation mdoel is more accurate on Iris data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=13, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the two types of model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 13)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is: 0.974\n",
      " \n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Testing accuracy is: %.3f\\n '% (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for each round: \n",
      " [1.         0.93333333 1.         1.         1.         0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "Average accuracy: 0.980 +- 0.0306\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 13)\n",
    "scores = cross_val_score(knn, X, y, cv = 10)\n",
    "print('Accuracy for each round: \\n', scores)\n",
    "print('Average accuracy: %.3f +- %.4f' % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6**: Can we use Multinomial Naive Bayes classifiation model for Iris data? Yes! We can discretize continuous features by use the [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer) method. Note that you can different ways of discrization. Please report the prediction result with the following discritization method. Also, try another discritization method with parameter 'encode=onehot', and report the prediction result. Use 10-cross validation to report the accuracy mean and standard deviation as we did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize the continous attributes with KBinSDiscretizer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "encoder = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "X = encoder.fit_transform(iris_data['data'])\n",
    "y = iris_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_mnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy is: %.2f\\n' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.3125     0.34821429 0.33928571]\n",
      "Estimated class-conditional probabilities for each feature: \n",
      " [[0.16666667 0.80555556 0.00925926 0.01851852]\n",
      " [0.23809524 0.16483516 0.31868132 0.27838828]\n",
      " [0.24096386 0.14939759 0.30120482 0.30843373]]\n"
     ]
    }
   ],
   "source": [
    "# Explore the learned model parameters (probabilities)\n",
    "# Note that the probabilities are in the logorithmic form. Why? The log-sum-exp trick for underflow of probability products\n",
    "print('Estimated probability of classess: \\n', np.e**clf_mnb.class_log_prior_)\n",
    "print('Estimated class-conditional probabilities for each feature: \\n', np.e**clf_mnb.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize the continous attributes with encode as onehot\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "encoder = KBinsDiscretizer(n_bins=5, encode='onehot', strategy='uniform')\n",
    "X = encoder.fit_transform(iris_data['data'])\n",
    "y = iris_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_mnb = MultinomialNB()\n",
    "clf_mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf_mnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy is: %.2f\\n' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated probability of classess: \n",
      " [0.3125     0.34821429 0.33928571]\n",
      "Estimated class-conditional probabilities for each feature: \n",
      " [[0.125      0.1        0.0125     0.00625    0.00625    0.0125\n",
      "  0.0125     0.1125     0.0875     0.025      0.225      0.00625\n",
      "  0.00625    0.00625    0.00625    0.21875    0.0125     0.00625\n",
      "  0.00625    0.00625   ]\n",
      " [0.02272727 0.09090909 0.08522727 0.04545455 0.00568182 0.05113636\n",
      "  0.10795455 0.07954545 0.00568182 0.00568182 0.00568182 0.02272727\n",
      "  0.14772727 0.06818182 0.00568182 0.00568182 0.04545455 0.16477273\n",
      "  0.02840909 0.00568182]\n",
      " [0.01162791 0.01744186 0.10465116 0.05813953 0.05813953 0.01162791\n",
      "  0.09302326 0.12209302 0.01744186 0.00581395 0.00581395 0.00581395\n",
      "  0.01162791 0.15697674 0.06976744 0.00581395 0.00581395 0.02325581\n",
      "  0.11627907 0.09883721]]\n"
     ]
    }
   ],
   "source": [
    "# Explore the learned model parameters (probabilities)\n",
    "# Note that the probabilities are in the logorithmic form. Why? The log-sum-exp trick for underflow of probability products\n",
    "print('Estimated probability of classess: \\n', np.e**clf_mnb.class_log_prior_)\n",
    "print('Estimated class-conditional probabilities for each feature: \\n', np.e**clf_mnb.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Predict Human Activity Recognition (HAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this practice exercise is to predict current human activity based on phisiological activity measurements from 53 different features based in the [HAR dataset](http://groupware.les.inf.puc-rio.br/har#sbia_paper_section). The training (`har_train.csv`) and test (`har_validate.csv`) datasets are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**: Build a Naive Bayes model, predict on the test dataset and compute the [confusion matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62). Note: Please refer to the [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). This is a check point question for this week's workshop. You need to report accuracy_scores on train and test set of Human Activity Recognition dataset. Also provide confusion matrix on test set and provide a brief interpretation of your results based on accuracy scores and confusion matrix (which class is misclassified into what). Your description should not be more than a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "har_train = pd.read_csv(\"files/har_train.csv\")\n",
    "har_test = pd.read_csv(\"files/har_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['classe', 'roll_belt', 'pitch_belt', 'yaw_belt', 'total_accel_belt',\n",
      "       'gyros_belt_x', 'gyros_belt_y', 'gyros_belt_z', 'accel_belt_x',\n",
      "       'accel_belt_y', 'accel_belt_z', 'magnet_belt_x', 'magnet_belt_y',\n",
      "       'magnet_belt_z', 'roll_arm', 'pitch_arm', 'yaw_arm', 'total_accel_arm',\n",
      "       'gyros_arm_x', 'gyros_arm_y', 'gyros_arm_z', 'accel_arm_x',\n",
      "       'accel_arm_y', 'accel_arm_z', 'magnet_arm_x', 'magnet_arm_y',\n",
      "       'magnet_arm_z', 'roll_dumbbell', 'pitch_dumbbell', 'yaw_dumbbell',\n",
      "       'total_accel_dumbbell', 'gyros_dumbbell_x', 'gyros_dumbbell_y',\n",
      "       'gyros_dumbbell_z', 'accel_dumbbell_x', 'accel_dumbbell_y',\n",
      "       'accel_dumbbell_z', 'magnet_dumbbell_x', 'magnet_dumbbell_y',\n",
      "       'magnet_dumbbell_z', 'roll_forearm', 'pitch_forearm', 'yaw_forearm',\n",
      "       'total_accel_forearm', 'gyros_forearm_x', 'gyros_forearm_y',\n",
      "       'gyros_forearm_z', 'accel_forearm_x', 'accel_forearm_y',\n",
      "       'accel_forearm_z', 'magnet_forearm_x', 'magnet_forearm_y',\n",
      "       'magnet_forearm_z'],\n",
      "      dtype='object')\n",
      "\n",
      " {'E', 'B', 'A', 'D', 'C'}\n"
     ]
    }
   ],
   "source": [
    "# checking all 53 column names in the dataset\n",
    "print(har_train.columns)\n",
    "\n",
    "# The variable to predict (or label) is \"classe\" column. \n",
    "# checking number of classes in classe column\n",
    "print(\"\\n\",set(har_train['classe']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A', 'B', 'C', 'D', 'E'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(har_train['classe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and class labels\n",
    "X_train = har_train.drop(['classe'], axis =1)\n",
    "y_train = har_train['classe']\n",
    "X_test = har_test.drop(['classe'], axis = 1)\n",
    "y_test = har_test['classe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the model\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is: 0.5543\n",
      " \n",
      "Training accuracy is: 0.5581 \n"
     ]
    }
   ],
   "source": [
    "# Reporting accuracy score and confusion matrix on test set\n",
    "y_pred_test = clf.predict(X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print('Testing accuracy is: %.4f\\n '% accuracy_test)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print('Training accuracy is: %.4f '% accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Confusion matrix on test set is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1070,   95,  262,  212,   35],\n",
       "       [ 127,  685,  145,   76,  106],\n",
       "       [ 223,  106,  512,  136,   49],\n",
       "       [ 102,   35,  271,  441,  115],\n",
       "       [  51,  239,   95,  143,  554]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "print('The Confusion matrix on test set is:')\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18e9696e248>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(confusion_matrix, cmap='binary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
